
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Learning Feature Representations - PRT Blog</title>
  <meta name="author" content="Kenneth Morton and Peter Torrione">

  
  <meta name="description" content="Learning Feature Representations Feb 27th, 2013 Today I wanted to go through an interesting paper I recently read and show how to implement parts of &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://newfolder.github.io/blog/2013/02/27/learning-feature-representations">
  <link href="/favicon.ico" rel="icon">
  
  <link href="/assets/bootstrap/css/spacelab.min.css" rel="stylesheet" type="text/css">
  <link href="/assets/bootstrap/css/bootstrap-responsive.min.css" rel="stylesheet" type="text/css">
  <link href="/assets/bootstrap/css/custom.css" rel="stylesheet" type="text/css">
  <link href="/assets/font-awesome/css/font-awesome.css" rel="stylesheet" type="text/css">
  
  <link href="/atom.xml" rel="alternate" title="PRT Blog" type="application/atom+xml">
  <style type="text/css">
pre, tt, code { font-size:12px; }
pre { margin:0px 0px 20px; }
pre.error { color:red; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }
</style>
  

</head>

<body    data-spy="scroll">

  <div class="container">
    <header class="jumbotron subhead" id="overview">
      
<div class="subscribe">
  <table>
    <tr>
      <td><span>Get Updates: &nbsp;</span></td>
      
      
      <td><a href="/atom.xml" class="btn"><i class="icon-cog"></i> By RSS</a></td>
      
      
    </tr>
  </table>
</div>

<h1 class="title">PRT Blog</h1>

  <p class="lead">MATLAB Pattern Recognition Open Free and Easy</p>


      <div class="navbar">
  <div class="navbar-inner">
    <div class="container" style="width: auto;">
      <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </a>
      <div class="nav-collapse">
                <ul class="nav">
          <li><a href="/">Home</a></li>
          <li><a href="/blog/archives">Blog</a></li>
          <li><a href="https://github.com/newfolder/PRT">Code</a></li>
          <li><a href="/prtdoc/">Documentation</a></li>
		  <li><a href="https://github.com/newfolder/PRT/issues">Get Help</a><li>
          <li><a href="/about">About</a></li>
        </ul>

        
          <form action="http://google.com/search" method="get" class="navbar-search pull-left">
            <fieldset role="search">
              <input type="hidden" name="q" value="site:newfolder.github.io" />
              <input type="text" name="q" results="0" placeholder="Search" class="search-query span2" />
            </fieldset>
          </form>
        
        
      </div><!-- /.nav-collapse -->
    </div>
  </div><!-- /navbar-inner -->
</div>

    </header>
    <div id="main">
      <div id="content">
        <div class="row">
  
  <div class="span8">
    <br>

  <header>
    
      <h1 class="entry-title">Learning Feature Representations</h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-02-27T14:59:00-05:00" pubdate data-updated="true">Feb 27<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Today I wanted to go through an interesting paper I recently read and show how to implement parts of that paper in the PRT.  The paper is <i>Learning Feature Representations with K-means</i> , by Adam Coates and Andrew Y. Ng (see below for full citation).</p>


<p>The meat of the Coates and Ng paper deals with how to use K-means to extract meaningful dictionaries from image data.  The latter part of the paper talks about how to do real machine learning with max-pooling for classification, but for today, I just wanted to introduce the MSRCORID (Microsoft Research Cambridge Object Recognition Image Database) data in the PRT and also show how to use the PRT to do some K-means dictionary learning.</p>




<!--/introduction-->


<h2>Contents</h2>


<div><ul><li><a href="#1">The MSRCORID Database</a></li><li><a href="#3">Extracting Patches</a></li><li><a href="#4">Normalization</a></li><li><a href="#5">K-Means</a></li><li><a href="#7">Simple Bag-Of-Words Classification</a></li><li><a href="#10">Bibliography</a></li></ul></div>


<h2>The MSRCORID Database<a name="1"></a></h2>


<p>For fun, I downloaded a new image database to play with for this data. The data is available for download from here: <a href="http://research.microsoft.com/en-us/downloads/b94de342-60dc-45d0-830b-9f6eff91b301/default.aspx">http://research.microsoft.com/en-us/downloads/b94de342-60dc-45d0-830b-9f6eff91b301/default.aspx</a></p>


<p>You can load the data automatically in the PRT, if you update to the newest version and run:</p>


<pre class="codeinput">ds = prtDataGenMsrcorid;
</pre>


<p>By default, that command will give a dataset with only images of chimneys and single flowers.  Look at the help for prtDataGenMsrcorid to see how to load data from these, and a lot more interesting classes.</p>


<p>You should note that prtDataGenMsrcorid does not output a prtDataSetClass, it produces a prtDataSetCellArray.  prtDataSetCellArray data objects are relatively new, and not fully documented, but they&#8217;re useful when you want to deal with datasets where each observation can have different sizes - e.g., images.</p>


<p>You can access elements using cell-array notation to access the .X field of the prtDataSet, for example:</p>


<pre class="codeinput">subplot(2,1,1);
imshow(ds.X{1});
title(<span class="string">'Flower'</span>);

subplot(2,1,2);
imshow(ds.X{end});
title(<span class="string">'Chimney'</span>);
</pre>


<p><img vspace="5" hspace="5" src="/images/torrione_ExampleCoatesNg_Kmeans_Mscorid_01.png" alt=""> <h2>Extracting Patches<a name="3"></a></h2><p>To generate a dictionary requires segmenting the initial images provided to us into sub-regions.  We can acheive this by using the MATLAB function im2col which with will convert every 8x8 sub-image to a 64x1 element vector.</p><pre class="codeinput">patchSize = [8 8];
col = [];
<span class="keyword">for</span> imgInd = 1:ds.nObservations;</p>

<pre><code>img = ds.X{imgInd};
img = rgb2gray(img);
img = imresize(img,.5);
col = cat(1,col,im2col(img,patchSize,&lt;span class="string"&gt;'distinct'&lt;/span&gt;)');
</code></pre>

<p><span class="keyword">end</span>
dsCol = prtDataSetClass(double(col));
</pre><h2>Normalization<a name="4"></a></h2><p>[Coates, 2012], makes it very clear that proper data normalization on a per-patch basis is fundamental to getting meaningful K-means centroids. The three main steps in the normalization are mean-normalization, energy normalization, and ZCA centering.  These are all implemented in the PRT as prtPreProcZeroMeanRows, prtPreProcStdNormalizeRows, and prtPreProcZca.</p><p>As always, we can buld an algorithm out of these independent components, then train and run the algorithm on the dsCol data we created earlier:</p><pre class="codeinput">preProc = prtPreProcZeroMeanRows + prtPreProcStdNormalizeRows(<span class="string">&lsquo;varianceOffset&rsquo;</span>,10) + prtPreProcZca;
preProc = preProc.train(dsCol);
dsNorm = preProc.run(dsCol);
</pre><h2>K-Means<a name="5"></a></h2><p>[Coates, 2012], makes a compelling case that K-means clustering is capable of learning dictionaries that can be easily used for classification.  The K-means algorithm in Coates paper is particularly intruiging, and its very fast compared to standard K-means using euclidean distances.  We&rsquo;ve implemented the K-means algorithm as described in [Coates, 2012] as prtClusterSphericalKmeans, which is much faster than using the regular K-means.</p><pre class="codeinput">skm = prtClusterSphericalKmeans(<span class="string">&lsquo;nClusters&rsquo;</span>,50);
skm = skm.train(dsNorm);
</pre><p>We can visualize the resulting cluster centers from the K-means processing by looking a the skm.clusterCenters, and plotting the first 50.  We&rsquo;ll sort these by how often data vectors were assigned to each cluster, so the top-left has the most elements, and the bottom-right has the least.</p><pre class="codeinput"></p>

<p>yOutK = skm.run(dsNorm);
[val,ind] = max(yOutK.X,[],2);
boolMat = zeros(size(yOutK.X));
indices = sub2ind(size(boolMat),(1:size(boolMat,1))&lsquo;,ind(:));
boolMat(indices) = 1;</p>

<p>clusterCounts = sum(boolMat);
[v,sortInds] = sort(clusterCounts,<span class="string">&lsquo;descend&rsquo;</span>);</p>

<p>c = skm.clusterCenters';
<span class="keyword">for</span> i = 1:50</p>

<pre><code>subplot(5,10,i);
imagesc(reshape(c(sortInds(i),:),patchSize));
title(v(i));
tickOff;
</code></pre>

<p><span class="keyword">end</span>
colormap <span class="string">gray</span>
</pre><img vspace="5" hspace="5" src="/images/torrione_ExampleCoatesNg_Kmeans_Mscorid_02.png" alt=""> <h2>Simple Bag-Of-Words Classification<a name="7"></a></h2><p>We can use some of the approaches from [Coates, 2012] to do some simple classification, also.  For example, we can use our new K-means clustering algorithm to generate features for each observation.  We can do this for every patch we extract from each image, but we&rsquo;d like to make decisions on an image-by-image basis, so we need to aggregate over the resulting feature vectors somehow.</p><p>A clever way to do this is to use max-pooling and deep-learning as specified in [Coates, 2012], but for now we&rsquo;ll just take the mean of the resulting feature vectors (in a manner similar to bag-of-words classification <a href="http://en.wikipedia.org/wiki/Bag-of-words_model"><a href="http://en.wikipedia.org/wiki/Bag-of-words_model">http://en.wikipedia.org/wiki/Bag-of-words_model</a></a> )</p><pre class="codeinput">featVec = nan(ds.nObservations,skm.nClusters);</p>

<p><span class="keyword">for</span> imgInd = 1:ds.nObservations;</p>

<pre><code>img = ds.X{imgInd};
img = rgb2gray(img);
col = im2col(img,patchSize,&lt;span class="string"&gt;'distinct'&lt;/span&gt;);
col = double(col);
dsCol = prtDataSetClass(col');
dsCol = run(preProc,dsCol);
dsFeat = skm.run(dsCol);
feats = max(dsFeat.X,.05);
featVec(imgInd,:) = mean(feats);
</code></pre>

<p><span class="keyword">end</span>
</pre><p>Now we can classify our feature vectors using another classification algorithm &ndash; e.g., here we use a SVM, with ZMUV pre-processing, and max-a-posteriori classification.</p><pre class="codeinput">dsFeat = prtDataSetClass(featVec,ds.targets);
dsFeat.classNames = ds.classNames;</p>

<p>yOut = kfolds(prtPreProcZmuv + prtClassLibSvm + prtDecisionMap,dsFeat,3);</p>

<p>close <span class="string">all</span>;
prtScoreConfusionMatrix(yOut)
</pre><img vspace="5" hspace="5" src="/images/torrione_ExampleCoatesNg_Kmeans_Mscorid_03.png" alt=""> <p>Hey!  That&rsquo;s not too bad for a few lines of code.  At some point in the future we&rsquo;ll take on the rest of the [Coates, 2012] paper, but in the meantime, let us know if you implement the max-pooling or other processes outlined therein.</p><p>Happy coding!</p><p>Note: we created prtPreProcZca, prtClusterSphericalKmeans, and prtDataGenMsrcorid for this blog entry; they&rsquo;re all in the PRT, but are recent (as of 2/27/2013) so download a new version to get access to all these.</p><h2>Bibliography<a name="10"></a></h2><p>Adam Coates and Andrew Y. Ng, Learning Feature Representations with K-means, G. Montavon, G. B. Orr, K.-R. Muller (Eds.), Neural Networks: Tricks of the Trade, 2nd edn, Springer LNCS 7700, 2012</p></p>
</div>


<br>
<hr>
<br>
    <footer>
      <p class="meta">
        
  

<span class="byline author vcard">Posted by <span class="fn">Pete</span></span>

        








  


<time datetime="2013-02-27T14:59:00-05:00" pubdate data-updated="true">Feb 27<span>th</span>, 2013</time>
        


      </p>
      
        <div class="sharing">
  <br/>
  
  
  
</div>

      
      <p class="meta">
        
          <a class="basic-alignment pull-left" href="/blog/2013/02/16/ieee-grss-hyperspectral-data/" title="Previous Post: IEEE GRSS Hyperspectral Data">&laquo; IEEE GRSS Hyperspectral Data</a>
        
        
          <a class="basic-alignment pull-right" href="/blog/2013/03/06/using-prtpath/" title="Next Post: Using prtPath">Using prtPath &raquo;</a>
        
      </p>
    </footer>
    
    
      <section>
        <h1>Comments</h1>
        <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
      </section>
    
  </div>

  
    
  <div class="span3 sidebar">
    <div class="well">
      
        <section>
  <h2>Recent Posts</h2>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2013/10/09/dude-wheres-my-help/">Dude Where's My Help?</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/09/04/verbosestorage-and-a-little-prtalgorithm-plotting/">verboseStorage and a little prtAlgorithm plotting</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/08/20/introducing-prtclassnnet/">Introducing prtClassNNET</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/07/29/supervised-learning/">Supervised Learning: An Introduction for Scientists and Engineers</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/07/24/using-svms/">Using SVMs for Scientists and Engineers</a>
      </li>
    
  </ul>
</section>


      
    </div>
  </div>


  
</div>


      </div>
    </div>
    <footer class="footer"><p>
  Copyright &copy; 2013 - Kenneth Morton and Peter Torrione -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span> -
  
    <span class="credit">Theme by <a href="http://brianarmstrong.org">Brian Armstrong</a></span>
  
</p>
</footer>
    

<script type="text/javascript">
      var disqus_shortname = 'newfolderconsulting';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://newfolder.github.io/blog/2013/02/27/learning-feature-representations/';
        var disqus_url = 'http://newfolder.github.io/blog/2013/02/27/learning-feature-representations/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
<script src="/assets/bootstrap/js/bootstrap.min.js"></script>


  </div>
</body>
</html>
